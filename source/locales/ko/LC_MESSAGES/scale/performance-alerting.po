# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2015-2025 Mattermost
# This file is distributed under the same license as the Mattermost package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Mattermost \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-05-15 06:10+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: ko\n"
"Language-Team: ko <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/scale/performance-alerting.rst:2
msgid "Mattermost performance alerting guide"
msgstr ""

#: ../../source/_static/badges/ent-cloud-selfhosted.rst:7
msgid ""
"|plans-img| Available on `Enterprise plans "
"<https://mattermost.com/pricing/>`__"
msgstr ""

#: ../../<rst_prolog>:2
msgid "plans-img"
msgstr ""

#: ../../source/_static/badges/ent-cloud-selfhosted.rst:9
msgid ""
"|deployment-img| `Cloud <https://mattermost.com/sign-up/>`__ and `self-"
"hosted <https://mattermost.com/download/>`__ deployments"
msgstr ""

#: ../../<rst_prolog>:4
msgid "deployment-img"
msgstr ""

#: ../../source/scale/performance-alerting.rst:7
msgid ""
"Mattermost recommends using `Prometheus <https://prometheus.io/>`_ and "
"`Grafana <https://grafana.com/>`_ to track performance metrics of the "
"Mattermost application servers. The purpose of this guide is to help you "
"set up alerts on your Grafana dashboard once you've :doc:`set up system "
"health tracking </scale/deploy-prometheus-grafana-for-performance-"
"monitoring>`."
msgstr ""

#: ../../source/scale/performance-alerting.rst:10
msgid ""
"We highly recommend setting up performance alerting for deployments above"
" 5,000 users, where additional servers have been added for performance "
"load-balancing."
msgstr ""

#: ../../source/scale/performance-alerting.rst:13
msgid "Prerequisites"
msgstr ""

#: ../../source/scale/performance-alerting.rst:15
msgid ""
"Set up performance monitoring for Mattermost. See our :doc:`Performance "
"Monitoring </scale/deploy-prometheus-grafana-for-performance-monitoring>`"
" documentation to learn more."
msgstr ""

#: ../../source/scale/performance-alerting.rst:17
msgid ""
"To get alerts, first set up a Notification Channel in Grafana. Here’s how"
" you can set it up to automatically post alerts in Mattermost:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:19
msgid "In Mattermost:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:21
msgid "Create an Alerts channel."
msgstr ""

#: ../../source/scale/performance-alerting.rst:22
msgid ""
"Create an `incoming webhook "
"<https://developers.mattermost.com/integrate/webhooks/incoming/>`_ for "
"the Alerts channel and copy the URL."
msgstr ""

#: ../../source/scale/performance-alerting.rst:24
msgid "In Grafana:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:26
msgid "Under the alert icon in the sidebar, select **Contact points**."
msgstr ""

#: ../../source/scale/performance-alerting.rst:27
msgid "Select **Create contact point**."
msgstr ""

#: ../../source/scale/performance-alerting.rst:28
msgid "Enter **Mattermost Alerts Channel** as the name."
msgstr ""

#: ../../source/scale/performance-alerting.rst:29
msgid "For type, select **Slack**."
msgstr ""

#: ../../source/scale/performance-alerting.rst:30
msgid "Paste your webhook URL into the URL field."
msgstr ""

#: ../../source/scale/performance-alerting.rst:31
msgid ""
"Include an @ mention in the mention field, if you want to send mentions "
"when an alert is posted to Mattermost."
msgstr ""

#: ../../source/scale/performance-alerting.rst:32
msgid "Press **Send Test** to test the alert."
msgstr ""

#: ../../source/scale/performance-alerting.rst:34
msgid ""
"If you would also like to get email alerts, you can follow `these "
"instructions <https://grafana.com/docs/grafana/latest/alerting/>`__ to "
"set that up."
msgstr ""

#: ../../source/scale/performance-alerting.rst:37
msgid "Configure alerts"
msgstr ""

#: ../../source/scale/performance-alerting.rst:39
msgid ""
"The `Mattermost dashboards "
"<https://grafana.com/grafana/dashboards/?search=mattermost>`_ for Grafana"
" come with some partially pre-configured alerts on the following charts:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:41
msgid "CPU Utilization Rate"
msgstr ""

#: ../../source/scale/performance-alerting.rst:42
msgid "Memory Usage"
msgstr ""

#: ../../source/scale/performance-alerting.rst:43
#: ../../source/scale/performance-alerting.rst:105
msgid "Number of Goroutines"
msgstr ""

#: ../../source/scale/performance-alerting.rst:44
msgid "Number of API Errors per Second"
msgstr ""

#: ../../source/scale/performance-alerting.rst:45
msgid "Mean API Request Time"
msgstr ""

#: ../../source/scale/performance-alerting.rst:47
msgid ""
"To configure alerts, set an appropriate threshold and enable "
"notifications. Enabling notifications is the same for each chart, but "
"setting the correct threshold can have some variances that are better "
"handled on a per-chart basis."
msgstr ""

#: ../../source/scale/performance-alerting.rst:49
msgid "For each chart, select the chart name, then select **Edit**:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:-1
msgid ""
"Configure Grafana dashboard alerts for each chart by selecting the chart "
"name then selecting Edit."
msgstr ""

#: ../../source/scale/performance-alerting.rst:54
msgid "Select the **Alert** tab:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:-1
msgid ""
"Switch to the Alert tab to access and configure Grafana dashboard alerts "
"for the current chart."
msgstr ""

#: ../../source/scale/performance-alerting.rst:59
msgid ""
"The alert threshold, which will be discussed in the sections below, is "
"the last field under **Conditions** (the one set to 600 in the screenshot"
" above)."
msgstr ""

#: ../../source/scale/performance-alerting.rst:61
msgid ""
"See the sections below for how to set the threshold for each individual "
"chart. If you would like to add your own custom alert conditions, "
"configure them here."
msgstr ""

#: ../../source/scale/performance-alerting.rst:-1
msgid ""
"When configuring Grafana dashboard alerts, set custom threshold "
"conditions in the Conditions section of the screen."
msgstr ""

#: ../../source/scale/performance-alerting.rst:66
msgid ""
"To enable the notifications for any alerts, select the **Notification** "
"tab on the left, then select **Mattermost Alerts Channel** under **Send "
"to**:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:-1
msgid ""
"When configuring Grafana dashboard alerts, set notifications for any "
"alerts by switching to the Notification tab, selecting Send to, then "
"selecting by Mattermost Alerts Channel."
msgstr ""

#: ../../source/scale/performance-alerting.rst:71
msgid "Enter a message if you would like to add more context to the alert."
msgstr ""

#: ../../source/scale/performance-alerting.rst:73
msgid ""
"By default, the alerts are configured to check the average of a chart "
"over the last minute to see if that value is above a threshold. If it’s "
"above the threshold, the alert will be triggered. Since it’s an average "
"over the last minute, small spikes that go past the threshold won’t "
"necessarily cause an alert. This helps prevent false positives that would"
" result from natural spikes in usage. The alert state for each chart is "
"evaluated every minute."
msgstr ""

#: ../../source/scale/performance-alerting.rst:76
msgid "Available charts"
msgstr ""

#: ../../source/scale/performance-alerting.rst:78
msgid "The sections below describe each chart in more detail."
msgstr ""

#: ../../source/scale/performance-alerting.rst:81
msgid "CPU utilization rate"
msgstr ""

#: ../../source/scale/performance-alerting.rst:83
#, python-format
msgid ""
"CPU Utilization Rate is fairly straightforward. CPU Utilization Rate "
"tracks the CPU usage of the app servers as a percentage. The maximum "
"percentage is based on the number of CPU cores or vCPUs your app server "
"has. For example, if you have four CPU cores and your app server was at "
"100% utilization rate on all four cores, the graph would show 400% for "
"that app server."
msgstr ""

#: ../../source/scale/performance-alerting.rst:85
msgid ""
"It’s best to set the alert threshold based on your average CPU "
"utilization and how many cores/vCPUs your app servers have. Take a look "
"at the chart over the last seven days. You’ll want to set the threshold "
"somewhere between your maximum CPU usage (cores * 100) and the CPU usage "
"you see. The lower you set the threshold, the more alerts, and therefore "
"the more false positives, you will get. Set the threshold too high and "
"you may not receive an alert for an incident until it’s gotten worse. The"
" same principle applies to all alerts, regardless of the chart."
msgstr ""

#: ../../source/scale/performance-alerting.rst:87
msgid "For example, on our community server, we have the threshold set to 15%:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:-1
msgid ""
"Example CPU utilization rate metrics for the Mattermost Community Server,"
" where the threshold is set to 15%. System admins should set the "
"threshold between the maximum CPU usage and the CPU usage observed in "
"metrics."
msgstr ""

#: ../../source/scale/performance-alerting.rst:92
msgid ""
"This value is below our maximum CPU usage and above our average usage at "
"peak times. Therefore, we will get alerts if we begin experiencing "
"unusually high CPU usage."
msgstr ""

#: ../../source/scale/performance-alerting.rst:95
msgid "Memory usage"
msgstr ""

#: ../../source/scale/performance-alerting.rst:97
msgid ""
"Memory Usage tracks the megabytes of RAM that your app servers are using."
" Set the threshold similar to the CPU Utilization Rate: below maximum "
"available memory and above your average usage during peak times."
msgstr ""

#: ../../source/scale/performance-alerting.rst:99
msgid "Here’s how we have the alert set on our Community server:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:-1
msgid ""
"Example memory usage metrics for the Mattermost Community Server, where "
"the threshold is configured similarly to the CPU utilization rate."
msgstr ""

#: ../../source/scale/performance-alerting.rst:107
msgid ""
"Goroutines are functions or methods that run concurrently with other "
"functions and methods. Goroutines are like lightweight threads with low-"
"creation costs. A rising number of goroutines can be a good measure of "
"the performance of your app servers. A continuous increase indicates your"
" app server can't keep up and is creating goroutines faster than they can"
" complete their tasks and stop."
msgstr ""

#: ../../source/scale/performance-alerting.rst:109
msgid ""
"Set the threshold somewhere above the average number of goroutines you "
"see during peak load times. Small spikes are usually nothing to worry "
"about. It’s the uncontrolled climbing of goroutines that you want to "
"watch out for."
msgstr ""

#: ../../source/scale/performance-alerting.rst:111
msgid "Here’s how we have it set on our Community server:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:-1
msgid ""
"Example Goroutines metrics for the Mattermost Community Server, where the"
" threshold is configured above the average number of Goroutines observed "
"during peak load times."
msgstr ""

#: ../../source/scale/performance-alerting.rst:117
msgid "Number of API errors per second"
msgstr ""

#: ../../source/scale/performance-alerting.rst:119
msgid ""
"Any 4xx or 5xx HTTP response status codes are counted as a REST API "
"error. API errors themselves are not necessarily a problem. There are "
"many legitimate reasons for an API error to occur, such as users’ "
"sessions expiring or clients requesting to see if a resource exists and "
"is being given a ``404 Not Found`` response. It is normal to have some "
"API errors that scale with your installation base."
msgstr ""

#: ../../source/scale/performance-alerting.rst:121
msgid ""
"That said, errors against the REST API can be indicative of deployment "
"and other issues. For example, if one of your app servers did not deploy "
"correctly for whatever reason, it may begin returning a high number of "
"API errors. Another example would be a rogue bot spamming the API with "
"bad requests. Alerts on API errors per second would help catch these and "
"other issues."
msgstr ""

#: ../../source/scale/performance-alerting.rst:123
msgid "Here’s how it’s set on our Community server:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:-1
msgid ""
"Example metrics of the number of API errors per second for the Mattermost"
" Community Server, where it's normal to have some API errors that scale "
"with an installation base, but that can be indicative of deployment "
"issues or other issues."
msgstr ""

#: ../../source/scale/performance-alerting.rst:129
msgid "Mean API request time"
msgstr ""

#: ../../source/scale/performance-alerting.rst:131
msgid ""
"The Mean API Request Time is the average amount of time a REST API "
"request to the Mattermost app server takes to complete. If an app server "
"starts to perform poorly, you’ll likely see a rise in the mean request "
"time as it takes longer to complete requests. This could also happen if "
"your database can’t sustain the load from the app servers. It may also be"
" indicative of an issue between the app servers and your proxy."
msgstr ""

#: ../../source/scale/performance-alerting.rst:133
msgid ""
"You’ll want to set the alert threshold a little above what the mean "
"request time is during your peak load times."
msgstr ""

#: ../../source/scale/performance-alerting.rst:135
msgid "Here’s how it’s set on our community server:"
msgstr ""

#: ../../source/scale/performance-alerting.rst:-1
msgid ""
"Example mean API request time metrics for the Mattermost Community "
"Server, where the alert threshold is configured a bit above the mean "
"request time during peak load times."
msgstr ""

#: ../../source/scale/performance-alerting.rst:141
msgid "Plugin hooks"
msgstr ""

#: ../../source/scale/performance-alerting.rst:143
msgid ""
"You can trace hooks and plugin API calls with Prometheus. Below are some "
"examples of hooks and API Prometheus metrics that you may want to be "
"aware of when troubleshooting or monitoring your server's performance."
msgstr ""

#: ../../source/scale/performance-alerting.rst:173
msgid "Other alerts"
msgstr ""

#: ../../source/scale/performance-alerting.rst:175
msgid ""
"If you want more alerts, you can set them up on any of the Grafana charts"
" you'd like. We recommend reviewing custom metrics listed on our "
":doc:`Performance Monitoring feature documentation </scale/performance-"
"monitoring-metrics>`."
msgstr ""

